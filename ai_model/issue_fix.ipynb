{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1. Setup"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "1.1 Importing Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import os\n",
                "import random\n",
                "import numpy as np \n",
                "from PIL import Image\n",
                "from matplotlib import pyplot as plt\n",
                "import uuid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow import keras\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
                "import tensorflow as tf"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "1.2 Set GPU Growth"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "# grabs all GPUs\n",
                "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
                "# sets memory growth for each GPU\n",
                "for gpu in gpus:\n",
                "    tf.config.experimental.set_memory_growth(gpu, True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "1.3 Create Folder Structures"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "POS_PATH = os.path.join(\"data\", \"positive\")\n",
                "NEG_PATH = os.path.join(\"data\", \"negative\")\n",
                "ANC_PATH = os.path.join(\"data\", \"anchor\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# make the directories\n",
                "os.makedirs(POS_PATH)\n",
                "os.makedirs(NEG_PATH)\n",
                "os.makedirs(ANC_PATH)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. Collect Positives and Anchors"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "2.1 Untar Labelled Faces in the Wild Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "# http://vis-www.cs.umass.edu/lfw/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# uncompress Tar GZ Labelled Faces in the Wild Dataset\n",
                "!tar -xf lfw.tgz"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# move LFW Images to the following repository data/negative\n",
                "for directory in os.listdir(\"lfw\"):\n",
                "    for file in os.listdir(os.path.join(\"lfw\", directory)):\n",
                "        EX_PATH = os.path.join(\"lfw\", directory, file)\n",
                "        NEW_PATH = os.path.join(NEG_PATH, file)\n",
                "        os.replace(EX_PATH, NEW_PATH)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "2.2 Collect Postitive and Anchor Images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import uuid (universally unique identifiers) library to generate unique image name \n",
                "import uuid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cap = cv2.VideoCapture(0)\n",
                "cap.set(3, 250)\n",
                "cap.set(4, 250)\n",
                "\n",
                "while True:\n",
                "    ret, frame = cap.read() # reads each frame one by one\n",
                "    \n",
                "    # create positives\n",
                "    if cv2.waitKey(1) & 0xFF == ord(\"p\"):\n",
                "        imname = os.path.join(POS_PATH, \"{}.jpg\".format(uuid.uuid1()))\n",
                "        cv2.imwrite(imname, frame)\n",
                "    # create anchors\n",
                "    if cv2.waitKey(1) & 0xFF == ord(\"a\"):\n",
                "        imname = os.path.join(ANC_PATH, \"{}.jpg\".format(uuid.uuid1()))\n",
                "        cv2.imwrite(imname, frame)\n",
                "\n",
                "\n",
                "\n",
                "    cv2.imshow(\"frame\", frame) # shows image \n",
                "    if cv2.waitKey(20) & 0xFF == 27: # allows keyboard press to close window with the escape key\n",
                "        break\n",
                "\n",
                "# when finished, release the capture\n",
                "cap.release() \n",
                "cv2.destroyAllWindows()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Establish a connection to the webcam\n",
                "cap = cv2.VideoCapture(0)\n",
                "while cap.isOpened(): \n",
                "    ret, frame = cap.read()\n",
                "   \n",
                "    # Cut down frame to 250x250pxqqqqqq\n",
                "    frame = frame[120:120+250,200:200+250, :]\n",
                "    \n",
                "    # Collect anchors \n",
                "    if cv2.waitKey(1) & 0XFF == ord(\"a\"):\n",
                "        # Create the unique file path \n",
                "        imgname = os.path.join(ANC_PATH, \"{}.jpg\".format(uuid.uuid1()))\n",
                "        # Write out anchor image\n",
                "        cv2.imwrite(imgname, frame)\n",
                "    \n",
                "    # Collect positives\n",
                "    if cv2.waitKey(1) & 0XFF == ord(\"p\"):\n",
                "        # Create the unique file path \n",
                "        imgname = os.path.join(POS_PATH, \"{}.jpg\".format(uuid.uuid1()))\n",
                "        # Write out positive image\n",
                "        cv2.imwrite(imgname, frame)\n",
                "    \n",
                "    # Show image back to screen\n",
                "    cv2.imshow(\"Image Collection\", frame)\n",
                "    \n",
                "    # Breaking gracefully\n",
                "    if cv2.waitKey(1) & 0XFF == ord(\"q\"):\n",
                "        break\n",
                "        \n",
                "# Release the webcam\n",
                "cap.release()\n",
                "# Close the image show frame\n",
                "cv2.destroyAllWindows()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Load and Pre-process Images"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "3.1 Get Image Directories"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "ename": "InvalidArgumentError",
                    "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: data\\\\anchor/*.jpg'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-17-3f4909ea0bcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# loads first 300 images in each of the directories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0manchor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mANC_PATH\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/*.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpositive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPOS_PATH\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/*.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnegative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNEG_PATH\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/*.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32mc:\\Users\\lukas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mlist_files\u001b[1;34m(file_pattern, shuffle, seed, name)\u001b[0m\n\u001b[0;32m   1382\u001b[0m           string_ops.reduce_join(file_pattern, separator=\", \"), name=\"message\")\n\u001b[0;32m   1383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m       assert_not_empty = control_flow_ops.Assert(\n\u001b[0m\u001b[0;32m   1385\u001b[0m           condition, [message], summarize=1, name=\"assert_not_empty\")\n\u001b[0;32m   1386\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0massert_not_empty\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32mc:\\Users\\lukas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32mc:\\Users\\lukas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mAssert\u001b[1;34m(condition, data, summarize, name)\u001b[0m\n\u001b[0;32m    154\u001b[0m       \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_n_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[0mdata_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_summarize_eager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m       raise errors.InvalidArgumentError(\n\u001b[0m\u001b[0;32m    157\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: data\\\\anchor/*.jpg'"
                    ]
                }
            ],
            "source": [
                "# loads first 300 images in each of the directories\n",
                "anchor = tf.data.Dataset.list_files(ANC_PATH+\"/*.jpg\").take(300)\n",
                "positive = tf.data.Dataset.list_files(POS_PATH+\"/*.jpg\").take(300) \n",
                "negative = tf.data.Dataset.list_files(NEG_PATH+\"/*.jpg\").take(300)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "3.2 Preprocessing - Scale and Resize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess(file_path):\n",
                "    # reading img\n",
                "    byte_img = tf.io.read_file(file_path)\n",
                "    # using tf decode image to load it in\n",
                "    img = tf.io.decode_jpeg(byte_img)\n",
                "    # resizes img\n",
                "    img = tf.image.resize(img, (100,100))\n",
                "    # scales image to be between 0 and 1\n",
                "    img = img / 255.0\n",
                "    return img"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "3.3 Create Labelled Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'anchor' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-10-d782e7a500cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# combines the anchor and positve/negative image. Adds 1.0/0.0 depending if same face\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpositives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnegatives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpositives\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnegatives\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'anchor' is not defined"
                    ]
                }
            ],
            "source": [
                "# combines the anchor and positve/negative image. Adds 1.0/0.0 depending if same face\n",
                "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
                "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
                "data = positives.concatenate(negatives)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "3.4 Build Train and Test Partition"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# puts input_img, validation_img and label into a list\n",
                "def preprocess_twin(input_img, validation_img, label):\n",
                "    return (preprocess(input_img), preprocess(validation_img), label)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# maps our data\n",
                "data = data.map(preprocess_twin)\n",
                "# caching our images so we can access them faster\n",
                "data = data.cache()\n",
                "# shuffles all our data,\n",
                "data = data.shuffle(buffer_size=1024)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Training ####\n",
                "# takes 70% of images for training data\n",
                "train_data = data.take(round(len(data)*.7))\n",
                "train_data = train_data.batch(32)\n",
                "# starts preprocessing the next set of images so that we don\"t bottle neck our next set images\n",
                "train_data = train_data.prefetch(8)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Testing ####\n",
                "# skips first 70% of images\n",
                "test_data = data.skip(round(len(data)*.7))\n",
                "# then takes 30% of data left \n",
                "test_data = test_data.take(round(len(data)*.3))\n",
                "test_data = test_data.batch(32)\n",
                "test_data = test_data.prefetch(8)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 4. Model Engineering"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "4.1 Build Embedding Layer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inp = Input(shape=(100,100,3), name=\"input_image\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inp = Input(shape=(100,100,3), name=\"input_image\")\n",
                "\n",
                "c1 = Conv2D(64, (10,10), activation=\"relu\")(inp)\n",
                "m1 = MaxPooling2D(64, (2,2), padding=\"same\")(c1)\n",
                "\n",
                "c2 = Conv2D(128, (7,7), activation=\"relu\")(m1)\n",
                "m2 = MaxPooling2D(64, (2,2), padding=\"same\")(c2)\n",
                "\n",
                "c3 = Conv2D(128, (4,4), activation=\"relu\")(m2)\n",
                "m3 = MaxPooling2D(64, (2,2), padding=\"same\")(c3)\n",
                "\n",
                "c4 = Conv2D(256, (4,4), activation=\"relu\")(m3)\n",
                "f1 = Flatten()(c4)\n",
                "d1 = Dense(4096, activation=\"sigmoid\")(f1)\n",
                "\n",
                "mod = Model(inputs=[inp], outputs=[d1], name=\"embedding\")\n",
                "mod.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_embedding():\n",
                "    inp = Input(shape=(100,100,3), name=\"input_image\")\n",
                "\n",
                "    #### First block ####\n",
                "    c1 = Conv2D(64, (10,10), activation=\"relu\")(inp)\n",
                "    m1 = MaxPooling2D(64, (2,2), padding=\"same\")(c1)\n",
                "\n",
                "    #### Second Block ####\n",
                "    c2 = Conv2D(128, (7,7), activation=\"relu\")(m1)\n",
                "    m2 = MaxPooling2D(64, (2,2), padding=\"same\")(c2)\n",
                "\n",
                "    #### Third Block ####\n",
                "    c3 = Conv2D(128, (4,4), activation=\"relu\")(m2)\n",
                "    m3 = MaxPooling2D(64, (2,2), padding=\"same\")(c3)\n",
                "\n",
                "    #### Final Embedding Block ####\n",
                "    c4 = Conv2D(256, (4,4), activation=\"relu\")(m3)\n",
                "    f1 = Flatten()(c4)\n",
                "    d1 = Dense(4096, activation=\"sigmoid\")(f1)\n",
                "    return Model(inputs=[inp], outputs=[d1], name=\"embedding\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "embedding = make_embedding()\n",
                "embedding.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Distance layer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class L1Dist(Layer):\n",
                "    def __init__(self, **kwargs):\n",
                "        super().__init__()\n",
                "        \n",
                "    def call(self, input_embedding, validation_embedding):\n",
                "        return tf.math.abs(input_embedding - validation_embedding)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Make model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_siamese_model():\n",
                "    input_image = Input(name=\"input_img\", shape=(100,100,3))\n",
                "    validation_image = Input(name=\"validation_img\", shape=(100,100,3))\n",
                "\n",
                "    \n",
                "    \n",
                "    siamese_layer = L1Dist()\n",
                "    siamese_layer._name = \"distance\"\n",
                "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
                "\n",
                "    classifier = Dense(1, activation=\"sigmoid\")(distances)\n",
                "    \n",
                "    return Model(inputs=[input_image, validation_image], outputs=classifier, name=\"SiameseNetwork\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "siamese_model = make_siamese_model()\n",
                "siamese_model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 5. Training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "5.1 Setup Loss and Optimiser"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# the loss will be used later to be able to calculate our loss (1 or 0)\n",
                "binary_loss = tf.losses.BinaryCrossentropy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# improves speed and performance\n",
                "opt = tf.keras.optimizers.Adam(1e-4) # 0.0001"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "5.2 Establish Checkpoints"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# defined our checkpoint dir\n",
                "checkpoint_dir = \"./training_checkpoints\"\n",
                "# ensures that all our checkpoints have the prefix of ckpt\n",
                "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
                "# saves our the model and optimiser at the time we run the checkpoint class\n",
                "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "5.3 Build Train Step Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@tf.function # compiles our function into a callable TensorFlow graph\n",
                "def train_step(batch):\n",
                "    # allows us to capture our gradient from the model, records the operations for automatic differentiation\n",
                "    with tf.GradientTape() as tape:\n",
                "        x = batch[:2] # get anchor and positive/negative images\n",
                "        y = batch[2] # takes the label\n",
                "\n",
                "        # passes our data through the siamese model to make a prediction\n",
                "        y_pred = siamese_model(x, training=True)\n",
                "        # calculates the loss\n",
                "        loss = binary_loss(y, y_pred)\n",
                "    # calculates all of the gradients in respect to our loss for all of our trainable variables   \n",
                "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
                "    # calculate updated weights and apply to siamese model\n",
                "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
                "\n",
                "    return loss\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "5.4 Build Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(data, EPOCHS):\n",
                "    for epoch in range(1, EPOCHS+1):\n",
                "        print(f\"\\n Epoch {epoch}/{EPOCHS}\")\n",
                "        progress_bar = tf.keras.utils.Progbar(len(data))\n",
                "\n",
                "    for idx, batch in enumerate(data):\n",
                "        train_step(batch)\n",
                "        progress_bar.update(idx+1)\n",
                "\n",
                "    if epoch % 10 == 0: \n",
                "        checkpoint.save(file_prefix=checkpoint_prefix)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "5.5 Train Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "EPOCHS = 50 # num times we will run through the training data\n",
                "train(train_data, EPOCHS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 6. Evaluate Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "siamese_model = tf.keras.models.load_model(\"siamese_model.h5\", custom_objects={\"L1Dist\":L1Dist, \"BinaryCrossentropy\":tf.losses.BinaryCrossentropy})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "6.1 Import Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Precision: Computes the precision of the predictions with respect to the labels\n",
                "# Recall: Computes the recall of the predictions with respect to the labels\n",
                "from tensorflow.keras.metrics import Precision, Recall "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "6.2 Make Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# unpacks batch\n",
                "test_input, test_validation, y_true = test_data.as_numpy_iterator().next() # converts our dataset as a numpy equivalent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# makes prediction\n",
                "y_pred = siamese_model.predict([test_input, test_validation])\n",
                "y_pred"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# if prediction is > 0.5, we want our result to add a 1\n",
                "results = []\n",
                "for prediction in y_pred:\n",
                "    if prediction > 0.5:\n",
                "        results.append(1)\n",
                "    else:\n",
                "        results.append(0)\n",
                "results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_true"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "6.3 Calculating Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# calculates recall metric\n",
                "metrics = Recall()\n",
                "metrics.update_state(y_true, y_pred) # calculating the recall value\n",
                "metrics = metrics.result().numpy()\n",
                "metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#  calculates precision metric\n",
                "metrics = Precision()\n",
                "metrics.update_state(y_true, y_pred)\n",
                "metrics = metrics.result().numpy()\n",
                "metrics"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "6.4 Visualise Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(18,8))\n",
                "\n",
                "plt.subplot(1,2,1)\n",
                "plt.imshow(test_input[0])\n",
                "\n",
                "plt.subplot(1,2,2)\n",
                "plt.imshow(test_validation[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 7. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#siamese_model.save(\"siamese_model.h5\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 8 Real test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "L1Dist"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = tf.keras.models.load_model(\"siamese_model.h5\", custom_objects={\"L1Dist\":L1Dist, \"BinaryCrossentropy\":tf.losses.BinaryCrossentropy})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def verify(model, detection_threshold, verification_threshold):\n",
                "    results = []\n",
                "    for image in os.listdir(os.path.join(\"application_data\", \"verification_images\")):\n",
                "        input_img = preprocess(os.path.join(\"application_data\", \"input_image\", \"input_img.jpg\"))\n",
                "        validation_img = preprocess(os.path.join(\"application_data\", \"verification_images\", image))\n",
                "\n",
                "        result = model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))\n",
                "        results.append(result)\n",
                "\n",
                "        detection = np.sum(np.array(results) > detection_threshold)\n",
                "\n",
                "        verification = detection / len(os.listdir(os.path.join(\"application_data\", \"verification_images\")))\n",
                "        \n",
                "        verified = verification > verification_threshold\n",
                "        return results, verified "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cap = cv2.VideoCapture(0)\n",
                "cap.set(3, 250)\n",
                "cap.set(4, 250)\n",
                "\n",
                "while True:\n",
                "    ret, frame = cap.read()\n",
                "\n",
                "    if cv2.waitKey(1) & 0xFF == ord(\"c\"):\n",
                "        imname = os.path.join(\"application_data\", \"input_image\", \"input_img.jpg\")\n",
                "        cv2.imwrite(imname, frame)\n",
                "\n",
                "        results, verified = verify(model, 0.1, 0.1)\n",
                "        print(verified)\n",
                "\n",
                "    cv2.imshow(\"verification\", frame) # shows image \n",
                "    if cv2.waitKey(20) & 0xFF == 27: # allows keyboard press to close window with the escape key\n",
                "        break\n",
                "\n",
                "cap.release() \n",
                "cv2.destroyAllWindows()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 9. Fix issue"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = tf.keras.models.load_model(\"siamese_model.h5\", custom_objects={\"L1Dist\":L1Dist, \"BinaryCrossentropy\":tf.losses.BinaryCrossentropy})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "9.1 Change images to same as test values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.preprocessing import image_dataset_from_directory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "INP_PATH = os.path.join(\"application_data\", \"input_image\", \"files\")\n",
                "VER_PATH = os.path.join(\"application_data\", \"verification_images\")\n",
                "NEG_PATH = os.path.join(\"application_data\", \"negative_images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_image = tf.data.Dataset.list_files(INP_PATH+\"/*.jpg\").take(300)\n",
                "validation_images = tf.data.Dataset.list_files(VER_PATH+\"/*.jpg\").take(300)\n",
                "negative_images = tf.data.Dataset.list_files(NEG_PATH+\"/*.jpg\").take(300)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess(file_path):\n",
                "    \n",
                "    # Read in image from file path\n",
                "    byte_img = tf.io.read_file(file_path)\n",
                "    # Load in the image \n",
                "    img = tf.io.decode_jpeg(byte_img)\n",
                "    \n",
                "    # Preprocessing steps - resizing the image to be 100x100x3\n",
                "    img = tf.image.resize(img, (100,100))\n",
                "    # Scale image to be between 0 and 1 \n",
                "    img = img / 255.0\n",
                "    \n",
                "    # Return image\n",
                "    return img"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pos_data = tf.data.Dataset.zip((input_image, validation_images, tf.data.Dataset.from_tensor_slices(tf.ones(len(input_image)))))\n",
                "neg_data = tf.data.Dataset.zip((input_image, negative_images, tf.data.Dataset.from_tensor_slices(tf.zeros(len(input_image))))) \n",
                "\n",
                "#data = image_dataset_from_directory(os.path.join(\"application_data\", \"input_image\"), batch_size=1, image_size=(250, 250))\n",
                "data = pos_data.concatenate(neg_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_twin(input_img, validation_img, label):\n",
                "    return(preprocess(input_img), preprocess(validation_img), label)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data = data.map(preprocess_twin)\n",
                "data = data.cache()\n",
                "data = data.shuffle(buffer_size=1024)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "real_data = data.take(round(len(data)*0.3))\n",
                "real_data = real_data.batch(32)\n",
                "real_data = real_data.prefetch(8)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "real_input, real_validation, real_label = real_data.as_numpy_iterator().next()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = model.predict([real_input, real_validation])\n",
                "y_pred"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# if prediction is > 0.5, we want our result to add a 1\n",
                "results = []\n",
                "for prediction in y_pred:\n",
                "    if prediction > 0.5:\n",
                "        results.append(1)\n",
                "    else:\n",
                "        results.append(0)\n",
                "results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "real_label"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Creating a metric object \n",
                "m = Recall()\n",
                "\n",
                "# Calculating the recall value \n",
                "m.update_state(y_true, y_pred)\n",
                "\n",
                "# Return Recall Result\n",
                "m.result().numpy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#from tensorflow.keras.preprocessing import image_dataset_from_directory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#real_data = image_dataset_from_directory(os.path.join(\"application_data\", \"input_image\"), batch_size=1, image_size=(250, 250))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "9.2 Implement into live design"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.preprocessing import image_dataset_from_directory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def data_change():\n",
                "    # takes path for files\n",
                "    INP_PATH = os.path.join(\"application_data\", \"input_image\", \"files\")\n",
                "    VER_PATH = os.path.join(\"application_data\", \"verification_images\")\n",
                "    NEG_PATH = os.path.join(\"application_data\", \"negative_images\")\n",
                "\n",
                "    # takes 50 images and puts them into dataset\n",
                "    input_image = tf.data.Dataset.list_files(INP_PATH+\"/*.jpg\").take(32)\n",
                "    validation_images = tf.data.Dataset.list_files(VER_PATH+\"/*.jpg\").take(32)\n",
                "    negative_images = tf.data.Dataset.list_files(NEG_PATH+\"/*.jpg\").take(32)\n",
                "\n",
                "    # ensures data is readable for the map function\n",
                "    #pos_data = image_dataset_from_directory(os.path.join(\"application_data\", \"input_image\"), batch_size=1, image_size=(250, 250))\n",
                "    \n",
                "    # creates our whole data (input_image, verification_image, label)\n",
                "    pos_data = tf.data.Dataset.zip((input_image, validation_images, tf.data.Dataset.from_tensor_slices(tf.ones(len(input_image)))))\n",
                "    neg_data = tf.data.Dataset.zip((input_image, negative_images, tf.data.Dataset.from_tensor_slices(tf.ones(len(input_image)))))\n",
                "    data = pos_data.concatenate(neg_data)\n",
                "\n",
                "    data = data.map(preprocess_twin)\n",
                "    data = data.cache()\n",
                "    #data = data.shuffle(buffer_size=1024)\n",
                "    data = data.take(32)\n",
                "    data = data.batch(32)\n",
                "    data = data.prefetch(8)\n",
                "\n",
                "    global real_input\n",
                "    global real_validation\n",
                "    real_input, real_validation, null = data.as_numpy_iterator().next()\n",
                "    # takes images from the dataset\n",
                "    #return real_input, real_validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def verify(model, detection_threshold, verification_threshold):\n",
                "    # changes data\n",
                "    data_change()\n",
                "    # predicts results\n",
                "    results = []\n",
                "    results2 = []\n",
                "    y_pred = model.predict([real_input, real_validation])\n",
                "\n",
                "    for prediction in y_pred:\n",
                "        if prediction > 0.1:\n",
                "            results2.append(1)\n",
                "        else:\n",
                "            results2.append(0)\n",
                "    print(results2)\n",
                "    results.append(y_pred)\n",
                "\n",
                "\n",
                "    \n",
                "    # Detection Threshold: Metric above which a prediciton is considered positive \n",
                "    detection = np.sum(np.array(results) > detection_threshold)\n",
                "    \n",
                "    # Verification Threshold: Proportion of positive predictions / total positive samples \n",
                "    verification = detection / len(os.listdir(os.path.join(\"application_data\", \"verification_images\"))) \n",
                "    verified = verification > verification_threshold\n",
                "    \n",
                "    return results, verified"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import uuid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cap = cv2.VideoCapture(0)\n",
                "while cap.isOpened():\n",
                "    ret, frame = cap.read()\n",
                "    frame = frame[120:120+250,200:200+250, :]\n",
                "    \n",
                "    cv2.imshow(\"Verification\", frame)\n",
                "    \n",
                "    # Verification trigger\n",
                "    if cv2.waitKey(1) & 0xFF == ord(\"v\"):\n",
                "        # removes existing images\n",
                "        for image in os.listdir(os.path.join(\"application_data\", \"input_image\", \"files\")):\n",
                "            image_remove = os.path.join(\"application_data\", \"input_image\", \"files\", image)\n",
                "            os.remove(image_remove)\n",
                "        \n",
                "        # Save input image to application_data/input_image folder \n",
                "        image_path = os.path.join(\"application_data\", \"input_image\", \"files\")\n",
                "        for i in range(32):\n",
                "            imgname = os.path.join(image_path, \"{}.jpg\".format(uuid.uuid1()))\n",
                "            cv2.imwrite(imgname, frame)\n",
                "        # Run verification\n",
                "        results, verified = verify(model, 0.5, 0.5)\n",
                "        print(verified)\n",
                "        break\n",
                "\n",
                "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
                "        break\n",
                "cap.release()\n",
                "cv2.destroyAllWindows()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_change()\n",
                "y_pred = model.predict([real_input, real_validation])\n",
                "y_pred"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "for prediction in y_pred:\n",
                "    if prediction > 0.5:\n",
                "        results.append(1)\n",
                "    else:\n",
                "        results.append(0)\n",
                "results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fix issue 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2022-11-02 11:07:56.202889: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
                        "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "2022-11-02 11:07:58.570633: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lukas/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
                        "2022-11-02 11:07:58.570664: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
                        "2022-11-02 11:07:58.612350: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
                        "2022-11-02 11:08:04.394836: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lukas/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
                        "2022-11-02 11:08:04.395069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lukas/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
                        "2022-11-02 11:08:04.395096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
                    ]
                }
            ],
            "source": [
                "import cv2\n",
                "import os\n",
                "import random\n",
                "import numpy as np \n",
                "from PIL import Image\n",
                "from matplotlib import pyplot as plt\n",
                "import uuid\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.metrics import Precision, Recall \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "class L1Dist(Layer):\n",
                "    def __init__(self, **kwargs):\n",
                "        super().__init__()\n",
                "        \n",
                "    def call(self, input_embedding, validation_embedding):\n",
                "        return tf.math.abs(input_embedding - validation_embedding)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2022-11-02 11:08:08.431796: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lukas/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
                        "2022-11-02 11:08:08.431861: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
                        "2022-11-02 11:08:08.431898: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lukas-HP-Pavilion-Notebook): /proc/driver/nvidia/version does not exist\n",
                        "2022-11-02 11:08:08.432273: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
                        "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "2022-11-02 11:08:08.556962: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 150994944 exceeds 10% of free system memory.\n",
                        "2022-11-02 11:08:08.724906: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 150994944 exceeds 10% of free system memory.\n",
                        "2022-11-02 11:08:08.770027: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 150994944 exceeds 10% of free system memory.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2022-11-02 11:08:09.472545: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 150994944 exceeds 10% of free system memory.\n"
                    ]
                }
            ],
            "source": [
                "siamese_model = tf.keras.models.load_model(\"siamese_model.h5\", custom_objects={\"L1Dist\":L1Dist, \"BinaryCrossentropy\":tf.losses.BinaryCrossentropy})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "POS_PATH = os.path.join(\"application_data\", \"input_image\")\n",
                "NEG_PATH = os.path.join(\"application_data\", \"negative_images\")\n",
                "ANC_PATH = os.path.join(\"application_data\", \"verification_images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Establish a connection to the webcam\n",
                "cap = cv2.VideoCapture(0)\n",
                "while cap.isOpened(): \n",
                "    ret, frame = cap.read()\n",
                "    # Cut down frame to 250x250px\n",
                "    frame = frame[120:120+250,200:200+250, :]\n",
                "    \n",
                "    # Collect anchors \n",
                "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
                "        for image in os.listdir(os.path.join(\"application_data\", \"input_image\")):\n",
                "            image_remove = os.path.join(\"application_data\", \"input_image\", image)\n",
                "            os.remove(image_remove)\n",
                "        for i in range(300):\n",
                "            # Create the unique file path \n",
                "            imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
                "            # Write out anchor image\n",
                "            cv2.imwrite(imgname, frame)\n",
                "        break\n",
                "    \n",
                "\n",
                "    cv2.imshow('Image Collection', frame)\n",
                "    \n",
                "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
                "        break\n",
                "        \n",
                "# Release the webcam\n",
                "cap.release()\n",
                "# Close the image show frame\n",
                "cv2.destroyAllWindows()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "anchor = tf.data.Dataset.list_files(ANC_PATH+\"/*.jpg\").take(300)\n",
                "positive = tf.data.Dataset.list_files(POS_PATH+\"/*.jpg\").take(300)\n",
                "negative = tf.data.Dataset.list_files(NEG_PATH+\"/*.jpg\").take(300)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess(file_path):\n",
                "    \n",
                "    # Read in image from file path\n",
                "    byte_img = tf.io.read_file(file_path)\n",
                "    # Load in the image \n",
                "    img = tf.io.decode_jpeg(byte_img)\n",
                "    \n",
                "    # Preprocessing steps - resizing the image to be 100x100x3\n",
                "    img = tf.image.resize(img, (100,100))\n",
                "    # Scale image to be between 0 and 1 \n",
                "    img = img / 255.0\n",
                "    \n",
                "    # Return image\n",
                "    return img"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
                "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
                "data = positives.concatenate(negatives)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_twin(input_img, validation_img, label):\n",
                "    return(preprocess(input_img), preprocess(validation_img), label)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "data = data.map(preprocess_twin)\n",
                "data = data.cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Testing partition\n",
                "test_data = data.skip(round(len(data)*.7))\n",
                "test_data = test_data.take(round(len(data)*.3))\n",
                "test_data = test_data.batch(32)\n",
                "test_data = test_data.prefetch(8)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2022-11-02 11:09:02.500319: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
                    ]
                }
            ],
            "source": [
                "test_input, test_validation, y_true = test_data.as_numpy_iterator().next()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1/1 [==============================] - 4s 4s/step\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "array([[0.00065651],\n",
                            "       [0.50116116],\n",
                            "       [0.00067683],\n",
                            "       [0.50116116],\n",
                            "       [0.50116116],\n",
                            "       [0.50116116],\n",
                            "       [0.50116116],\n",
                            "       [0.50116116],\n",
                            "       [0.50116116],\n",
                            "       [0.50116116],\n",
                            "       [0.50116116],\n",
                            "       [0.50116116],\n",
                            "       [0.50116116],\n",
                            "       [0.50116116],\n",
                            "       [0.50116116],\n",
                            "       [0.50116116]], dtype=float32)"
                        ]
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "y_pred = siamese_model.predict([test_input, test_validation])\n",
                "y_pred"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
                        ]
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "results = []\n",
                "for prediction in y_pred:\n",
                "    if prediction < 0.6:\n",
                "        results.append(1)\n",
                "    else:\n",
                "        results.append(0)\n",
                "results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.875"
                        ]
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Creating a metric object \n",
                "m = Recall()\n",
                "# Calculating the recall value \n",
                "m.update_state(y_true, y_pred)\n",
                "# Return Recall Result\n",
                "m.result().numpy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "True\n"
                    ]
                }
            ],
            "source": [
                "if m.result().numpy() >= 0.875:\n",
                "    print(\"True\")\n",
                "else:\n",
                "    print(\"False\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]"
        },
        "vscode": {
            "interpreter": {
                "hash": "cb9fa36926a9e0b41c3df3c5f18bb5922c8d139d0a67a86d29e170776fb60892"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}